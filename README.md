Somesh_kaggle
Group Activity
<br>
#Natural Language Processing with Disaster Tweets

Data Cleaning,EDA--SOMESH GAWANDE 

Identifying missing values, duplicates, null value then count the no.value_counts about text no.of character,no.of words & no.sentence . finally display the plot about count.


Additional EDA--ANIL KOKARE

Text Preprocessing:  Transforming raw data into format that is easier to understand and process by algorithms. It involves: Lowercasing- convert all text to lowercase e.g. "Hello"->"hello" Tokenization- split the text into smaller units such as words. Removing Punctuation- Eliminate special characters and punctuation e.g. "hello!"->"hello" Removing stop words- Eliminate common words like "and","the","in",etc that don't add much meaning Stemming- reduce words to their root form e.g. "running"->"run" OR Lemmatization- converts word to their base form with meaning.

Data Preprocessing --ASHWIN PARKHANDE

Model Building (ASHWIN PARKHANDE AND SOMESH GAWANDE  ): Use stratified data splitting to train the two models LogisticRegression and Multinomial NaiveBayes to model the data. Although their accuracy were close but 
LogisticRegression performed better, and therefore it was chosen as our final model.

Conclusion:
"Our results show that Logistic Regression outperforms other models like Multinomial NaiveBayes in terms of accuracy, achieving the accuracy of around 81%. Therefore, Logistic Regression was choosen as our final model for this dataset."
